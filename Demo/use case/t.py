"""
Filename: ml_modeling_debug.py
Note: ä¿®å¤äº†å¼‚æ­¥æ‰§è¡Œé—®é¢˜ï¼Œå¹¶å¢åŠ äº†è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—å’Œ Ollama å¥åº·æ£€æŸ¥ã€‚
"""
import fire
import sys
import os
import asyncio
import socket
import openai.resources.chat.completions
from metagpt.roles.di.data_interpreter import DataInterpreter
from metagpt.config2 import Config
from metagpt.logs import logger

# ===================================================================
# 0. ğŸ› ï¸ è°ƒè¯•é…ç½® (å¼€å¯è¯¦ç»†æ—¥å¿—)
# ===================================================================
# ç§»é™¤é»˜è®¤æ—¥å¿—å¤„ç†å™¨ï¼Œæ·»åŠ  DEBUG çº§åˆ«çš„å¤„ç†å™¨
logger.remove()
logger.add(sys.stderr, level="DEBUG")
logger.info("ğŸ› DEBUG æ¨¡å¼å·²å¼€å¯ï¼šå°†æ˜¾ç¤ºæ‰€æœ‰åº•å±‚é€šä¿¡ç»†èŠ‚")

# ===================================================================
# 1. ğŸš‘ è¡¥ä¸ï¼šOpenAI å‚æ•°å…¼å®¹æ€§ä¿®å¤
# ===================================================================
_original_create = openai.resources.chat.completions.AsyncCompletions.create

async def _patched_create(self, *args, **kwargs):
    if "max_tokens" in kwargs:
        max_tokens = kwargs.pop("max_tokens")
        if max_tokens is not None:
            kwargs["max_completion_tokens"] = max_tokens
    return await _original_create(self, *args, **kwargs)

openai.resources.chat.completions.AsyncCompletions.create = _patched_create
logger.info("âœ… å‚æ•°å…¼å®¹æ€§è¡¥ä¸å·²åŠ è½½")

# ===================================================================
# 2. âš™ï¸ é…ç½®ä¸ç¯å¢ƒæ£€æŸ¥
# ===================================================================

# 2.1 æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦å¯åŠ¨
def check_ollama_port(host="127.0.0.1", port=11434):
    logger.info(f"ğŸ” æ­£åœ¨æ£€æŸ¥ Ollama æœåŠ¡ ({host}:{port})...")
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(2)
    result = sock.connect_ex((host, port))
    sock.close()
    if result == 0:
        logger.info("âœ… Ollama æœåŠ¡è¿æ¥æˆåŠŸï¼")
        return True
    else:
        logger.error("âŒ æ— æ³•è¿æ¥åˆ° Ollamaï¼è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œäº† 'ollama serve' æˆ– App å·²æ‰“å¼€ã€‚")
        return False

if not check_ollama_port():
    sys.exit(1)

# 2.2 é…ç½® Config
ollama_config = Config.from_llm_config({
    "api_type": "ollama",
    "base_url": "http://127.0.0.1:11434/api",
    "model": "qwen2.5-coder:7b", 
})

# 2.3 è·¯å¾„æ£€æŸ¥
DATA_DIR = "/Users/richsion/Desktop/MetaGPT/MetaGPT-Ewan/dataset/Walmart_Sales_Forecast"
TRAIN_FILE = f"{DATA_DIR}/train.csv"

if not os.path.exists(TRAIN_FILE):
    logger.error(f"âŒ è‡´å‘½é”™è¯¯ï¼šæœ¬åœ°æ‰¾ä¸åˆ°æ–‡ä»¶ï¼è·¯å¾„: {TRAIN_FILE}")
    sys.exit(1)
else:
    logger.info(f"âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {TRAIN_FILE}")

# ===================================================================
# 3. ğŸ“ å¼ºåˆ¶æ€§ Prompt
# ===================================================================
SALES_FORECAST_REQ = f"""
**ROLE**: You are a Python Data Scientist using Qwen-Coder.

**URGENT INSTRUCTION**: 
The user has provided a local dataset. You MUST use the ABSOLUTE PATH provided below.
**NEVER** generate fake paths like 'path_to_data.csv'.

**DATASET PATH**: `{TRAIN_FILE}`

**STEP 1: LOAD DATA (COPY THIS CODE EXACTLY)**
Write and execute the following Python code to start. Do not change the path.
```python
import pandas as pd
# Load data using the absolute path
try:
    df = pd.read_csv(r'{TRAIN_FILE}')
    print("Data loaded successfully!")
    print(f"Columns: {{df.columns.tolist()}}")
    print(df.head())
except Exception as e:
    print(f"Load failed: {{e}}")

STEP 2: ANALYSIS & MODELING After loading the data:

Preprocess the 'Date' column to datetime objects.

Split the data: use the last 40 weeks as the validation set, and the rest as the training set.

Train a model (e.g., RandomForest) to predict 'Weekly_Sales'.

Evaluate using WMAE.

Plot the total sales trends.

OUTPUT REQUIREMENT:

Logs must be in English. """

REQUIREMENTS = {"sales_forecast": SALES_FORECAST_REQ}

# ===================================================================
# 4. ğŸš€ ä¸»ç¨‹åº (å¼‚æ­¥å°è£…)
# ===================================================================
async def main_async(use_case: str): 
    logger.info("ğŸš€ DataInterpreter æ­£åœ¨åˆå§‹åŒ–...") 
    try: 
        mi = DataInterpreter(config=ollama_config) 
        logger.info("ğŸ¤– Agent åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æ¥æ”¶ä»»åŠ¡...")

        requirement = REQUIREMENTS[use_case]
        logger.debug(f"Prompt å‘é€å†…å®¹é¢„è§ˆ: {requirement[:100]}...")
        
        await mi.run(requirement)
        logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼")
    
    except Exception as e:
        logger.exception(f"ğŸ’¥ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}")
def entrypoint(use_case: str = "sales_forecast"): 
    """ åŒæ­¥å…¥å£å‡½æ•°ï¼Œç”¨äº Fire è°ƒç”¨ï¼Œå†…éƒ¨è´Ÿè´£å¯åŠ¨å¼‚æ­¥å¾ªç¯ã€‚ """ 
    logger.info(f"ğŸ”¥ ç¨‹åºå¯åŠ¨ï¼Œå½“å‰ Use Case: {use_case}") 
    try: 
        asyncio.run(main_async(use_case)) 
    except KeyboardInterrupt: 
        logger.warning("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ç¨‹åº") 
    except Exception as e: 
        logger.exception(f"ä¸»ç¨‹åºå´©æºƒ: {e}")

if __name__ == "main": 
    fire.Fire(entrypoint)